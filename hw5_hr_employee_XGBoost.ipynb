{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw7_hr_employee(student).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pataweepr/applyML_vistec_2019/blob/master/hw5_hr_employee_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "A7ilUYmKU3ax",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# HR Employee Attrition prediction using XGBoost\n",
        "\n",
        "In this lab, we will work on the task of employee attrition prediction. We will predict whether an employee will leave the job or not. The model we will use is XGBoost which is a kind of random forest. One key feature for XGBoost is that it can handle missing values by learning the default path to take when the data is null. Moreover, since it is a decision tree in nature, it is highly interpretable while maintaining high performance. In this lab, we will learn the way to tune, and analyze the model to get insights about what the model is doing.\n",
        "\n",
        "The data is modified from the [Kaggle tutorial](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset)."
      ]
    },
    {
      "metadata": {
        "id": "aFSxezMKbgCu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import mode\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import metrics\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from numpy import random \n",
        "\n",
        "seed = 5\n",
        "random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DTdeVA7hQtjA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data can be acquired as usual. Get the data [here](https://drive.google.com/file/d/1iWb3YTeCddIhqAeRvhW6z9YnhQFYX4_8/view?usp=sharing) by clicking add to drive."
      ]
    },
    {
      "metadata": {
        "id": "gqLmKi0nifIS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "By8WGnL0kqoa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read the data file. Study the names of the features. There are two columns that should be dropped. \n",
        "\n",
        "Which ones?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "LHt4KWh-jQJY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hr_data = pd.read_csv(\"/content/gdrive/My Drive/hr-employee-attrition.csv\")\n",
        "## TODO#1 ##\n",
        "# Drop two useless columns\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kZytGZPpmFC8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The column we will predict is the Attrition. Change Attrition==No to 0 and Attrition==Yes to 1."
      ]
    },
    {
      "metadata": {
        "id": "QTMfarh8k4BZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#2 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JQnp27HDzExY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Is there an imbalance in the prediction colum? What is the number of people who left? What is the number of people who stay?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "PjDOvS-AEi5V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How many cells have null values?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "c1gQP0rimf0k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below are the same functions we used before. \n",
        "\n",
        "Function splitTrainTest splits train, validation, and test function in a stratified manner.\n",
        "\n",
        "Funciton get_feature_groups split the data into numerical features and categorical features."
      ]
    },
    {
      "metadata": {
        "id": "TqkcidHliYb6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def splitTrainTest(data,ratio_train_valid_test,name_label):\n",
        "    keyDatas = data[name_label].value_counts().keys()\n",
        "    train = pd.DataFrame()\n",
        "    valid = pd.DataFrame()\n",
        "    test = pd.DataFrame()\n",
        "    first_ratio = ratio_train_valid_test[2]/np.sum(ratio_train_valid_test)\n",
        "    second_ratio =  ratio_train_valid_test[1]/np.sum(ratio_train_valid_test[:2])\n",
        "    for k in keyDatas:\n",
        "        tmp = data[data[name_label]==k]\n",
        "        tmp_train, tmp_test = train_test_split(tmp, test_size = first_ratio, random_state=seed)\n",
        "        tmp_train, tmp_valid = train_test_split(tmp_train, test_size = second_ratio, random_state=seed)\n",
        "        train = train.append(tmp_train)\n",
        "        valid = valid.append(tmp_valid)\n",
        "        test = test.append(tmp_test)\n",
        "    train.reset_index(drop=True)\n",
        "    valid.reset_index(drop=True)\n",
        "    test.reset_index(drop=True)\n",
        "    return train, valid, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pC6WXI40Uo94",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_feature_groups(ames_df):\n",
        "    # Numerical Features\n",
        "    numberical_features = ames_df.select_dtypes(include=['int64','float64']).columns\n",
        "    # We drop ID and SalePrice since these are not input features\n",
        "    numberical_features = numberical_features.drop('Attrition') \n",
        "\n",
        "    # Categorical Features\n",
        "    catagorical_features = ames_df.select_dtypes(include=['object']).columns\n",
        "    return list(numberical_features), list(catagorical_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tSsGP1ECH4HP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data exploration"
      ]
    },
    {
      "metadata": {
        "id": "_5Xs1mELbhPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code below changes the strings in categorical features into [categorical data](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html)"
      ]
    },
    {
      "metadata": {
        "id": "zI-J7d_HUwlu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_features, cat_features = get_feature_groups(hr_data)\n",
        "\n",
        "print('numberical feature')\n",
        "print(num_features)\n",
        "print('----------------------------------')\n",
        "print('catagorical feature')\n",
        "print(cat_features)\n",
        "\n",
        "for col in cat_features:\n",
        "    hr_data[col] = pd.Categorical(hr_data[col])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ug1ebXFcw5p6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Numerical features analysis\n",
        "\n",
        "The code below shows the distribution of the numerical features."
      ]
    },
    {
      "metadata": {
        "id": "HenOqYdZxBgX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = pd.melt(hr_data, value_vars=sorted(num_features))\n",
        "g = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\n",
        "g = g.map(sns.distplot, 'value')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xtviuiIlrylf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create two sets of plots. One using only the data from employees who left. The other using the data from employees who stayed.\n",
        "\n",
        "What features show the biggest differences? Does it make sense?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "Suggest other methods to study the difference between leave and stay besides looking at the distributions.\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "aJHpJWDorxoQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#3 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rWvfPg6NxAt_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Catagorical features analysis\n",
        "\n",
        "The code below shows the distribution of the categorical features."
      ]
    },
    {
      "metadata": {
        "id": "D9rhkM8BxHlt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = pd.melt(hr_data, value_vars=sorted(cat_features))\n",
        "g = sns.FacetGrid(f, col='variable', col_wrap=3, sharex=False, sharey=False)\n",
        "plt.xticks(rotation='vertical')\n",
        "g = g.map(sns.countplot, 'value')\n",
        "[plt.setp(ax.get_xticklabels(), rotation=60) for ax in g.axes.flat]\n",
        "g.fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UDV3o1vluSXf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Identify at least two useless features in the data (from both categorical and numerical) Remove them.\n",
        "\n",
        "** Ans: ** "
      ]
    },
    {
      "metadata": {
        "id": "QEDRRhMaF2f9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#4 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p4c6hx81xP_e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Change categorical features into numbers\n",
        "\n",
        "We need to change categorical features into numbers. There are many ways to do this. In this lab, we will one-hot encoded our features.\n",
        "\n",
        "** One hot encoding ** is a method that change categories into a binary vector where the category correponds to the index where the vector is one. For example, for the gender feature we change \"male\" to \\[1, 0\\] and \"female\" to \\[0, 1\\].\n",
        "\n",
        "Pandas has a nice function [pandas.get_dummies](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) that can help use do this.\n",
        "\n",
        "For a more advance but extremely robust method to change categories to numbers, see the optional section."
      ]
    },
    {
      "metadata": {
        "id": "Psn3dN3aGf4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#5 ##\n",
        "# Change the categorical features using one hot encoding\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aMD7pbx3ylTX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "for feature in cat_features:\n",
        "  # Create one-hot feature columns for Max_EduInstituteGroup, HireType, HireSourceGroup\n",
        "  df_onehot = pd.get_dummies(hr_data[feature], prefix=feature)\n",
        "  # Concat the new columns to the dataframe\n",
        "  hr_data = pd.concat([hr_data, df_onehot], axis=1)\n",
        "  hr_data = hr_data.drop(columns = feature)\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "hxRRc85KQBy3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(hr_data.shape)\n",
        "print(hr_data.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-mx3m3RQAet",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Down sampling\n",
        "\n",
        "In order to deal with class imbalance, we will down sample the data. This can be done by randomly removing samples from that majority class. The function below down samples the data."
      ]
    },
    {
      "metadata": {
        "id": "BBo1BOPyP_v7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def down_sampling(X,ratio_neg_pos):\n",
        "  X_pos = X.loc[X['Attrition'] == 1].reset_index(drop=True)\n",
        "  X_neg = X.loc[X['Attrition'] == 0].reset_index(drop=True)\n",
        "  ## shuffle\n",
        "  np_rand_ind = np.array(X_neg.index)\n",
        "  random.shuffle(np_rand_ind)\n",
        "  size_neg = int(ratio_neg_pos*X_pos.shape[0])\n",
        "  np_rand_ind = np_rand_ind[:size_neg]\n",
        "  X_neg = X_neg.iloc[np_rand_ind]\n",
        "  X_down_samp = pd.concat([X_pos,X_neg])\n",
        "  X_down_samp = shuffle(X_down_samp,random_state = seed)\n",
        "  X_down_samp.reset_index(drop=True)\n",
        "  return X_down_samp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f2b3BL7bT5D3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ratio = np.array([70,20,10])\n",
        "train_set, valid_set, test_set = splitTrainTest(hr_data,ratio,'Attrition')\n",
        "\n",
        "print(train_set.shape)\n",
        "print(valid_set.shape)\n",
        "print(test_set.shape)\n",
        "print('---------')\n",
        "# We do not need the ratio to be 1:1 but they should be in the same order of magnitude\n",
        "ratio_neg_pos = 1.5\n",
        "train_set = down_sampling(train_set,ratio_neg_pos)\n",
        "print(train_set.shape)\n",
        "print(valid_set.shape)\n",
        "print(test_set.shape)\n",
        "\n",
        "X_train = train_set.drop(columns = ['Attrition'])\n",
        "y_train = train_set['Attrition']\n",
        "\n",
        "X_valid = valid_set.drop(columns = ['Attrition'])\n",
        "y_valid = valid_set['Attrition']\n",
        "\n",
        "X_test = test_set.drop(columns = ['Attrition'])\n",
        "y_test = test_set['Attrition']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8pWmEcN6dxgh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train_set.columns)\n",
        "train_set.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKvJ8h6_z4M_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalize the data\n",
        "\n",
        "Use minmax scaler to normalize the features in the train, validation, and test set."
      ]
    },
    {
      "metadata": {
        "id": "aagi1I2k68Ch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#6 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xtEfSB1nLjuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## XGBoost\n",
        "\n",
        "[XGBoost](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster) stands for Extreme Gradient Boosting. It is a kind of random forest model that can be used for classification and regression. The gradient boosting term refers to kind of machine learning model that is created by combining weak classifiers. Unlike random forests, XGBoost trees are usually shallower. It has built in feature selection capability just like random forests.\n",
        "\n",
        "The code below is an example of a XGBoost model."
      ]
    },
    {
      "metadata": {
        "id": "fcc5sQLRhBuV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = xgb.XGBClassifier(silent=False, \n",
        "                          scale_pos_weight=ratio_neg_pos,\n",
        "                          learning_rate=0.01,  \n",
        "                          colsample_bytree = 0.4,\n",
        "                          subsample = 0.8,\n",
        "                          min_child_weight = 2.5 ,\n",
        "                          objective='binary:logistic', \n",
        "                          n_estimators=1000,\n",
        "                          reg_alpha = 0.3,\n",
        "                          max_depth=5, \n",
        "                          gamma=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jJ6C30EE1c_E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**What are the meaning of each settings listed above?** You do not need to answer this, but you should try to understand its implications."
      ]
    },
    {
      "metadata": {
        "id": "zwWbCBF1N-Gv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the validation set\n",
        "eval_set = [(X_valid_minmax,y_valid)]\n",
        "eval_metric = ['auc','error']\n",
        "\n",
        "# train the model\n",
        "model.fit(X_train_minmax, y_train,eval_metric=eval_metric,eval_set=eval_set,verbose =True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FhUnCiD2lZ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use the model to classify the test set, e.g. *.predict()* . Also get probability values, e.g. *.predict_proba()*, so that we can create an RoC curve."
      ]
    },
    {
      "metadata": {
        "id": "SEsSOAbx2kuB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#6 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RNkn0YYU3tK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "y_pred_test_prob = model.predict_proba(X_test_minmax)[:,1]\n",
        "y_pred_test = model.predict(X_test_minmax)\n",
        "print(y_pred_test)\n",
        "print(y_pred_test_prob)\n",
        "print(y_test.values)\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "McPUihmWIlH4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What is the threshold of that probability that the model will answer class 0?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "We will re-visit this when we look into the RoC curve."
      ]
    },
    {
      "metadata": {
        "id": "4g9WCebz3yw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a confusion matrix. \n",
        "\n",
        "Which class is harder to predict? What do you think makes this class harder?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "Y2klS0HZ381N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#7 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Id5xCjCcGfBS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Metrics for imbalance classification\n",
        "\n",
        "Next we will compare different kinds of metrics, namely accuracy, precision, and recall.\n",
        "\n",
        "The code below calculates different metrics on the test set. Do you know the difference between rows and columns?"
      ]
    },
    {
      "metadata": {
        "id": "UjuxmVkVG1e5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('acc : ',accuracy_score(y_test.values, y_pred_test))\n",
        "tg_name = ['Attrition : no','Attrition : yes']\n",
        "print(classification_report(y_test.values,y_pred_test,target_names=tg_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V5IyBfEjHLET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you always answer the majority class (Attribution = no), what is the accuracy, recall (for attribution = yes), and precision?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "It turns out that the majority class answer has higher accuracy but zero recall. ** This shows how accuracy might not be the best metric for when there is high class imbalance. **\n",
        "\n",
        "Note that we are still missing one important piece. The threshold value for our prediction."
      ]
    },
    {
      "metadata": {
        "id": "STVuwNzCHKcv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#8 ##\n",
        "# Calculate accuracy for majority class prediction\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sW75Ji2m36BE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RoC\n",
        "\n",
        "Next we will look at the RoC curve. The code below plots the RoC curve."
      ]
    },
    {
      "metadata": {
        "id": "owdkTaXII7_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(y_test.values, y_pred_test_prob, pos_label=1)\n",
        "thresholds = np.array(thresholds)[::-1]\n",
        "\n",
        "plt.plot(fpr,tpr)\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3BoruhUsJCg4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we really care about the accuracy, we can maximize the accuracy by varying the threshold. Modify the code below to find the threshold that maximizes the accuracy.\n",
        "\n",
        "What are the best threshold and best accuracy values?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "Note: we should do this by tuning on the validation set rather than the test set."
      ]
    },
    {
      "metadata": {
        "id": "Y2HG1IZ9JinE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#8 ##\n",
        "## MODIFY THIS CODE ##\n",
        "for thes in thresholds:\n",
        "  print('threshold : ', thes)\n",
        "  y_pred_test_th = y_pred_test_prob >= thes\n",
        "  y_pred_test_th = list(y_pred_test_th.astype(int))\n",
        "  conf_test = confusion_matrix(y_test.values, y_pred_test_th)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OomOCVxV8w47",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tree visualization\n",
        "\n",
        "We can visualize the trees in XGBoost to get a sense on what the model is doing, by using the code below."
      ]
    },
    {
      "metadata": {
        "id": "A1rmqznpQqMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# View tree\n",
        "# Because graphviz seams to have some resolution problem with pyplot and jupyter, we have to use to trick below to get a readable graph\n",
        "fig, ax = plt.subplots(figsize=(5, 5), dpi=350)\n",
        "\n",
        "# Plot the n-th tree of the model. Note that the model consists of many tress (as defined when creating the model)\n",
        "xgb.plot_tree(model, num_trees=15, rankdir='LR', ax=ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ny2sd2Y6KwlZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature importance\n",
        "\n",
        "Another useful feature is to look at how often each feature is used in the trees. Feature importance of a single tree signifies how a feature helps seperate the classes in the tree. The feature importance of every tree can be averaged and shown in a single plot below:"
      ]
    },
    {
      "metadata": {
        "id": "7a_zwF-0Ri51",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xgb.plot_importance(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dHDDdTkwL6tr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What are the top four features with highest importance? Do they make sense? Do they agree with your analysis earlier (in the data exploration section)?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "You can read more about how feature importance is calculated in [Matthew Drury's answer](https://stats.stackexchange.com/questions/162162/relative-variable-importance-for-boosting)"
      ]
    },
    {
      "metadata": {
        "id": "Z6j3thFYvl-v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (Optional): Target encoding\n",
        "\n",
        "Besides one hot encoding, there are other ways to change categorical features to numbers.\n",
        "\n",
        "The most popular methods are based on target encoding, which change the value of each category to the average value of the class you want to predict. More advanced versions involve smoothing and prior values so that the encoding will not overfit. You can read more about target encoding [here](https://maxhalford.github.io/blog/target-encoding-done-the-right-way/). Another more rigorous source can be found (here)[https://dl.acm.org/citation.cfm?id=507538]\n",
        "\n",
        "Perform any target encoding variants of your choice and re-do the XGBoost training and compare the difference."
      ]
    },
    {
      "metadata": {
        "id": "jN0LrTTwNFaS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "emwhznX0NO1C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (Optional): XGBoost tuning\n",
        "\n",
        "There are many hyperparameters in XGBoost which can be cumbersome to tune. This [blog](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python) describes tips and tricks for tuning the hyperparameters.\n",
        "\n",
        "Try tuning the hyperparameter to get better classification results."
      ]
    },
    {
      "metadata": {
        "id": "1A0Pgfj2NODz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}