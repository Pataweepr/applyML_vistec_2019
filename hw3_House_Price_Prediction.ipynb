{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3_House_Price_Prediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pataweepr/applyML_vistec_2019/blob/master/hw3_House_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "G6CGOezHSexx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Housing Price prediction using linear regression\n",
        "\n",
        "This homework uses data from [Kaggle's House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n",
        "\n",
        "**Goal**\n",
        "\n",
        "The goal of this homework is to predict the sales price for each house."
      ]
    },
    {
      "metadata": {
        "id": "ki92RGEezm_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import library \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score,mean_squared_error\n",
        "from google.colab import files\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import display\n",
        "import json\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JB-2smOqKTjw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get the data into your google drive by accessing this [link](https://drive.google.com/open?id=1jgEPVsZ8CWjoKDrD0VB75Jb0igPCU9vc) and click add to drive."
      ]
    },
    {
      "metadata": {
        "id": "ye4x4gpbD2kq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97HFwuM0D4NG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip '/content/gdrive/My Drive/house-prices-advanced-regression-techniques.zip'\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sXT5niImK4Fb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You will notice multiple files. Here are the main ones:\n",
        "\n",
        "1.   data_description.txt explains the data\n",
        "2.   train.csv contains the training data\n",
        "3.   test.csv contains the test set for Kaggle\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "puqlVWvDLJTE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Spliting the data\n",
        "\n",
        "For convenience we will ignore the test set of the Kaggle competition and create our own validation set using the training data.\n",
        "\n",
        "Split the training data into training and validation set with 1:10 proportion.\n",
        "\n",
        "Use [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to do so. Set the random state to 30, so that we can reproduce the same split every time we re-run the code."
      ]
    },
    {
      "metadata": {
        "id": "axfgDziwL_PU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#1 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okeSH3KtM7RH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "train_data = pd.read_csv('train.csv')\n",
        "train_data, val_data = train_test_split(train_data, test_size=1/11, random_state=30)\n",
        "test_data = pd.read_csv('test.csv')\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "cXC6wUfiMKv4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Try looking around the data using pandas.summary, pandas.head, and pandas.tail.\n",
        "\n",
        "What is the number of training data?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "What is the number of validation data?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "What is the number of test data?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "How many features are in the data?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "0QqbtbriEjcf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cleaning the data\n",
        "\n",
        "Fill the missing values in the training and validation data for features:\n",
        "\n",
        "1.   LotFrontage\n",
        "2.   GarageYrBlt\n",
        "3.  MasVnrArea\n",
        "\n",
        "with their mode values. (Remember to use the mode values from the training data not the validation data)."
      ]
    },
    {
      "metadata": {
        "id": "yLYJw5wfFsvC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#2 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ywIxukE1-ilU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "train_data[\"LotFrontage\"] = train_data[\"LotFrontage\"].fillna(train_data[\"LotFrontage\"].mode().iloc[0])\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "r_OMVme0Plu1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data exploration and visualization\n",
        "\n",
        "Since we have many variables in this dataset, we will try to visualize it using  [sns.FacetGrid](https://seaborn.pydata.org/generated/seaborn.FacetGrid.html) via histograms.\n",
        "\n",
        "However, to use FacetGrid, we need to put our data into [pd.melt](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html) format.\n",
        "\n",
        "Look at an example on how to do so [here](https://seaborn.pydata.org/tutorial/axis_grids.html)."
      ]
    },
    {
      "metadata": {
        "id": "d2J0UUcQQa9K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before we can plot histograms, we first need to know which feature columns are categorical and which feature columns are numerical. \n",
        "\n",
        "The function below takes in the training data frame and will return two lists. One includes the name of categorical features and the other one containing numerical features."
      ]
    },
    {
      "metadata": {
        "id": "g-R_zviufirX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_feature_groups(ames_df):\n",
        "    # Numerical Features\n",
        "    num_features = ames_df.select_dtypes(include=['int64','float64']).columns\n",
        "    # We drop ID and SalePrice since these are not input features\n",
        "    num_features = num_features.drop(['Id','SalePrice']) \n",
        "\n",
        "    # Categorical Features\n",
        "    cat_features = ames_df.select_dtypes(include=['object']).columns\n",
        "    return list(num_features), list(cat_features)\n",
        "  \n",
        "num_features, cat_features = get_feature_groups(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Wwo5xsYRfu-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#3 ##\n",
        "## Plot the two group of featuers. Use sns.distplot for the numerical features\n",
        "## and snsl.countplot for the categorical features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uGggzLIzTFWf"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "# Grid of distribution plots of all numerical features\n",
        "f = pd.melt(train_data, value_vars=sorted(num_features))\n",
        "g = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\n",
        "g = g.map(sns.distplot, 'value')\n",
        "\n",
        "# Grid of frequency plots of all categoriccal features\n",
        "f = pd.melt(train_data, value_vars=sorted(cat_features))\n",
        "g = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\n",
        "plt.xticks(rotation='vertical')\n",
        "g = g.map(sns.countplot, 'value')\n",
        "[plt.setp(ax.get_xticklabels(), rotation=60) for ax in g.axes.flat]\n",
        "g.fig.tight_layout()\n",
        "plt.show()\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "sOjUWEcvSyKr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the plots, name one feature that should not be helpful for predicting price? Why? (A useless feature is one that does not contain any information.)\n",
        "\n",
        "** Ans: **\n"
      ]
    },
    {
      "metadata": {
        "id": "t4QNDcvwULbc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we will look into the distribution of house prices. Create a histogram of the SalePrice with 100 bins."
      ]
    },
    {
      "metadata": {
        "id": "NmHr4EX9UrMU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#4 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Woz5hX4eUzzS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Write down any observation you have from the histogram. What direction is the tail of the distribution?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "j_rynXpGVCuP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One of the problems we can notice is that there are extremely high values that can cause problems for any prediction model.\n",
        "\n",
        "One way to normalize data with high spread is to squish it using a logarithmic function.\n",
        "\n",
        "Normalize the price values using the function:\n",
        "\n",
        "$y=log(1+x)$\n",
        "\n",
        "*The addition by one is so that we will not have problems if x = 0.*\n",
        "\n",
        "Plot the histogram of the price after the log normalization.\n",
        "\n",
        "Note: Numpy includes a function that does log(1+x) for you called [np.log1p](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log1p.html). The inverse of the function is called [np.expm1](https://docs.scipy.org/doc/numpy/reference/generated/numpy.expm1.html#numpy.expm1).\n"
      ]
    },
    {
      "metadata": {
        "id": "Lq0O2ro-UzPY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#5 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YDpD8j85WiO4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compare and contrast this new histogram with the previous one.\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "JEK7J4HIU_rk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the next sections we will start building models. Before we get into that, we created a normalizer function for you to use."
      ]
    },
    {
      "metadata": {
        "id": "RKPG1jvR5PqV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# If you want to scale the SalePrice, set is_one_dim to False.\n",
        "def normalizer(data,is_one_dim = False):\n",
        "  if (is_one_dim):\n",
        "    min_max_scale = preprocessing.MinMaxScaler().fit(data.reshape((len(data),1)))\n",
        "  else:\n",
        "    min_max_scale = preprocessing.MinMaxScaler().fit(data)\n",
        "  return min_max_scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yMkRj4CrVIDe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also have a function that evaluate the prediction in various ways.\n",
        "\n",
        "Function inputs:\n",
        "*   *Predict_data_train_norm* is the predicted price (normzlied using log1p and min-max scaled)\n",
        "*   *real_price_train* is the unnormalized true price\n",
        "*   *p_normalizer* is the min-max scaler for the price data\n",
        "\n",
        "Function outputs:\n",
        "\n",
        "*   *RMSE* is the Root Mean Square Error of the predicted price.\n",
        "*   *Mean percentage error* is the mean of the prediction error in percent (relative to the correct price).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZC6xSwpgvqrU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_lin_reg(predict_data_train_norm,predict_data_val_norm,real_price_train,real_price_val,p_normalizer):\n",
        "  \n",
        "  train_predict_log1p = p_normalizer.inverse_transform(predict_data_train_norm.reshape((len(predict_data_train_norm),1))).reshape(len(predict_data_train_norm))\n",
        "  train_predict = np.expm1(train_predict_log1p)\n",
        "  \n",
        "  val_predict_log1p = p_normalizer.inverse_transform(predict_data_val_norm.reshape((len(predict_data_val_norm),1))).reshape(len(predict_data_val_norm))\n",
        "  val_predict = np.expm1(val_predict_log1p)\n",
        "  \n",
        "  rmse_train = np.sqrt(mean_squared_error(train_predict,real_price_train))\n",
        "  mean_per_train = np.mean(np.absolute(train_predict - real_price_train)/real_price_train)*100\n",
        "  \n",
        "  rmse_val = np.sqrt(mean_squared_error(val_predict,real_price_val))\n",
        "  mean_per_val = np.mean(np.absolute(val_predict - real_price_val)/real_price_val)*100\n",
        "  \n",
        "  \n",
        "  print('rmse train set : ', rmse_train)\n",
        "  print('mean percentage error train set : ', mean_per_train)\n",
        "  print('rmse val set : ', rmse_val)\n",
        "  print('mean percentage error val set : ', mean_per_val)\n",
        "  \n",
        "  return rmse_train,mean_per_train,rmse_val,mean_per_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k8wGTl_TbeQ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Regression model\n",
        "\n",
        "In this section we will use linear regression to do housing price prediction.\n",
        "\n",
        "See [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) for examples on how to use linear regression.\n",
        "\n",
        "Normalize the input features using min-max normalizer, and normalize the price (output target) using log1p followed by a min-max normalizer.\n",
        "\n",
        "Use the function evaluate_lin_reg() to report your results."
      ]
    },
    {
      "metadata": {
        "id": "bmcYRENyb6yV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#6 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKFUVvcY_UbR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "train_np = np.array(train_data[num_features].values)\n",
        "train_price_data = np.array(train_data['SalePrice'].values)\n",
        "train_log1p_price = np.log1p(train_price_data)\n",
        "train_normalizer = normalizer(train_np)\n",
        "train_np_norm = train_normalizer.transform(train_np)\n",
        "price_normalizer = normalizer(train_log1p_price,is_one_dim = True)\n",
        "train_log1p_price_norm = price_normalizer.transform(train_log1p_price.reshape(1,len(train_log1p_price))).reshape(len(train_log1p_price))\n",
        "\n",
        "lin_reg = LinearRegression().fit(train_np_norm, train_log1p_price_norm)\n",
        "train_predict_log1p_norm = lin_reg.predict(train_np_norm)\n",
        "\n",
        "val_np = np.array(val_data[num_features].values)\n",
        "val_price_data = np.array(val_data['SalePrice'].values)\n",
        "val_log1p_price = np.log1p(val_price_data)\n",
        "val_np_norm = train_normalizer.transform(val_np)\n",
        "val_log1p_price_norm = price_normalizer.transform(val_log1p_price.reshape(1,len(val_log1p_price))).reshape(len(val_log1p_price))\n",
        "\n",
        "val_predict_log1p_norm = lin_reg.predict(val_np_norm)\n",
        "\n",
        "rms_train,mp_train,rms_val,mp_val = evaluate_lin_reg(train_predict_log1p_norm,val_predict_log1p_norm,train_price_data,val_price_data,price_normalizer)\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "-GDCZ35bcwni",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to get a sense of how good our model is we can compare it against baselines.\n",
        "\n",
        "One such baseline is a model that always output the mean of the SalePrice. What is the RMSE of the sale price predicted this way?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "Compute the Standard deviation of the SalePrice, you will find that it is very close to the RMSE value. Explain why.\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "HTEv_ju5WL8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature selection\n",
        "\n",
        "One of the important factor for good classification results is the quality of our input features.\n",
        "\n",
        "One way to look at feature quality is the calculate the correlation coefficients (np.corrcoef from HW1).\n",
        "\n",
        "Calculate the correlation between each data column in the training data (including the salePrice). This is called a correlation matrix.\n",
        "\n",
        "Visualize it using [sns.heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n"
      ]
    },
    {
      "metadata": {
        "id": "7lbjr8_D96N6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#7 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s8di2Lw5_eJI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "data_to_sel_feature = np.vstack((train_np,val_np))\n",
        "log1p_price_all = np.hstack((train_log1p_price,val_log1p_price))\n",
        "\n",
        "data_train = np.hstack((data_to_sel_feature,log1p_price_all.reshape((len(log1p_price_all),1))))\n",
        "print(data_train.shape)\n",
        "\n",
        "corre_mat = np.corrcoef(data_train.T)\n",
        "ax = sns.heatmap(corre_mat, vmin=0, vmax=1)\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "zxAVUTuS42Fc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sort the input columns by the correlation coefficient with the SalePrice. \n",
        "\n",
        "What feature has the highest correlation with the SalePrice? Does it make sense?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "N5mIHOjuWlHt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#8 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xjm-KWF2_y2v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "corre_mat_abs = np.absolute(corre_mat[36,:36])\n",
        "ind_feature_imp = np.flip(np.argsort(corre_mat_abs) ,axis = 0)\n",
        "num_features_np = np.array(num_features)\n",
        "print(num_features_np[ind_feature_imp])\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "H88WNqdJyI-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Feature selection using correlation values (univariate analysis)\n",
        "\n",
        "Use only 6 features that have the highest correlation value to train a new model. Compare the RMSE (both training and validation) with the previous model. Why? (Think about overfitting and underfitting)\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "n2JU5KF3W5eY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#9 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sF4gnNIMy0RK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "# sort by correlation\n",
        "num_feature_sel = 6\n",
        "feature_sel = num_features_np[ind_feature_imp[:num_feature_sel]]\n",
        "\n",
        "train_cut_np = np.array(train_data[feature_sel].values)\n",
        "train_cut_normalizer = normalizer(train_cut_np)\n",
        "train_cut_norm = train_cut_normalizer.transform(train_cut_np)\n",
        "\n",
        "lin_reg_cut = LinearRegression().fit(train_cut_norm, train_log1p_price_norm)\n",
        "train_cut_predict_log1p_norm = lin_reg_cut.predict(train_cut_norm)\n",
        "\n",
        "val_cut_np = np.array(val_data[feature_sel].values)\n",
        "val_cut_norm = train_cut_normalizer.transform(val_cut_np)\n",
        "val_cut_predict_log1p_norm = lin_reg_cut.predict(val_cut_norm)\n",
        "\n",
        "rms_train,mp_train,rms_val,mp_val = evaluate_lin_reg(train_cut_predict_log1p_norm,val_cut_predict_log1p_norm,train_price_data,val_price_data,price_normalizer)\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "yC81YELuy-AI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Feature selection using machine learing models (multivariate analysis)\n",
        "\n",
        "Selecting the right features can have a huge impact on the performance of the model. In the previous section, we measure the correlation between a *single* variable with the prediction target. This disregards the interaction with other features and can lead to non-optimal solutions. \n",
        "\n",
        "Another method to measure the importance of features is to look at the coefficients $w_i$ of the regression model.\n",
        "\n",
        "$y = \\Sigma_i x_i * w_i$\n",
        "\n",
        "where $i$ is the feature index.\n",
        "\n",
        "The size of the weights can tell how important that feature is. Since the weight is calculated in conjunction with other features, it also take into account of other features.\n",
        "\n",
        "Rank the features by the **absolute value** of the regression weights. Compare it with the ranking given by the correlation coefficient.\n",
        "\n",
        "You can access the weights by looking at LinearRegression.coef_ in a trained model.\n",
        "\n",
        "Which makes more sense?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "wRUjHLCq1-5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#10 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XUrMnOEK4WBs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use only 6 features that have the highest absolute weight value to train a new model. Compare the RMSE (both training and validation) with the previous model."
      ]
    },
    {
      "metadata": {
        "id": "EigOEljPxljV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#11 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sgsgog2hWZfb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Non-linear features\n",
        "\n",
        "Linear regression is a linear model, meaning the output of the model is a linear function (a straight plane) wrt. the input features.\n",
        "\n",
        "In order to make linear regression more powerful, we usually add non-linear features by adding higher order features. The additional features are usually features, for example, if we add polynomial of degree 2 to features $\\{x_1, x_2\\}$, we will get the features $\\{x_1^2, x_2^2, x_1x_2, x_1, x_2\\}$.\n",
        "\n",
        "Sk-learn have a function that help you create [polynomial feature](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
        "\n",
        "From the 6 features in the previous step, construct polynomial features of degree 2 with only interaction terms (no $x_1^2$ and $x_2^2$ terms). Then, train a linear regression model using those non-linear features.\n",
        "\n",
        "Compare the RMSE (both training and validation) with the previous model."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zoCbOnvMGd_p",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#11 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XTriN7QqAMQW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "train_poly_np = preprocessing.PolynomialFeatures(2,interaction_only=True).fit_transform(train_cut_np)\n",
        "train_poly_normalizer = normalizer(train_poly_np)\n",
        "train_poly_norm = train_poly_normalizer.transform(train_poly_np)\n",
        "\n",
        "lin_reg_poly = LinearRegression().fit(train_poly_norm, train_log1p_price_norm)\n",
        "train_poly_predict_log1p_norm = lin_reg_poly.predict(train_poly_norm)\n",
        "\n",
        "val_poly_np = preprocessing.PolynomialFeatures(2,interaction_only=True).fit_transform(val_cut_np)\n",
        "val_poly_norm = train_poly_normalizer.transform(val_poly_np)\n",
        "val_poly_predict_log1p_norm = lin_reg_poly.predict(val_poly_norm)\n",
        "\n",
        "rms_train,mp_train,rms_val,mp_val = evaluate_lin_reg(train_poly_predict_log1p_norm,val_poly_predict_log1p_norm,train_price_data,val_price_data,price_normalizer)\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "sKXtRr-v4uvy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Overftting\n",
        "\n",
        "With higher order features, it is easy to overfit to the training data. \n",
        "\n",
        "From the previous step, now construct polynomial features of degree 2 with **all terms**. Then, train a linear regression model using those non-linear features.\n",
        "\n",
        "Compare the RMSE (both training and validation) with the previous model.\n",
        "\n",
        "In practice, the features used (feature selection) and the polynomial terms are hyperparameter you need to tune using the validation set. "
      ]
    },
    {
      "metadata": {
        "id": "hAlyY_M02jko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#12 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AHINxFXU7jJK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Elastic Net and Lasso\n",
        "\n",
        "In this part of the homework we will explore the use of L1 regularization. In machine learning we can reduce the overfitting by introducing regularization terms. This is accomplished by changing the objective function to force desirable properties.\n",
        "\n",
        "There are two kinds of regularization that is often used, L1 and L2 regularizations.\n",
        "\n",
        "Lasso uses L1 reguarlization, while Elastic Net use both L1 and L2 at the same time.\n",
        "\n",
        "An interesting property of L1 regularization is that it can be used to select only meaningful features. The features that are not useful will be ignore (the associated weights are zero). \n",
        "\n",
        "Create a linear regression model using polynomial features of degree 3.\n",
        "\n",
        "Note: you will not be able to evaluate the model using RMSE because you will be getting NaN values. Diagnose the problem by looking at the histogram of the predicted log1p normalized values of the validation set. Cap the predicted values to a reasonable value and evaluate the model."
      ]
    },
    {
      "metadata": {
        "id": "7vRQtt5ffG-7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#13 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z8lQc4A-2aDb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Plot a histogram of the absolute values of the weights used in the model."
      ]
    },
    {
      "metadata": {
        "id": "Pb6Pz5r8yjJw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#14 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z5smyWZI6sls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) to perform L1 regularization. Set alpha = 0.00005 and l1_ratio = 1. This setting turns the Elastic Net into a Lasso model. Evaluate the model. Compare the RMSE (both training and validation) with the previous model."
      ]
    },
    {
      "metadata": {
        "id": "rDgsS5KBsZ-X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#15 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ffxlbWA_Ar5u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "train_poly_np = preprocessing.PolynomialFeatures(3).fit_transform(train_np)\n",
        "train_poly_normalizer = normalizer(train_poly_np)\n",
        "train_poly_norm = train_poly_normalizer.transform(train_poly_np)\n",
        "\n",
        "val_poly_np = preprocessing.PolynomialFeatures(3).fit_transform(val_np)\n",
        "val_poly_norm = train_poly_normalizer.transform(val_poly_np)\n",
        "\n",
        "alpha_sel = 0.00005\n",
        "l1_ratio_sel = 1.0\n",
        "\n",
        "lin_reg_l1 = ElasticNet(alpha = alpha_sel, l1_ratio = l1_ratio_sel).fit(train_poly_norm, train_log1p_price_norm)\n",
        "train_predict_l1_log1p_norm = lin_reg_l1.predict(train_poly_norm)\n",
        "\n",
        "val_predict_l1_log1p_norm = lin_reg_l1.predict(val_poly_norm)\n",
        "\n",
        "rms_train,mp_train,rms_val,mp_val = evaluate_lin_reg(train_predict_l1_log1p_norm,val_predict_l1_log1p_norm,train_price_data,val_price_data,price_normalizer)\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "lzFwgDKw8PDU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Comparing model weights between models with regularization and without\n",
        "\n",
        "Finally we will look at the weights of the Lasso model. Plot a histogram of the weights and compare it to the model wihtout the regularization. Describe how this is related to feature selection.\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "WScE3o3TgVzk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#16 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}