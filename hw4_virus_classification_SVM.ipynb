{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_virus_classification_SVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pataweepr/applyML_vistec_2019/blob/master/hw4_virus_classification_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "oPS1-2OQUBeZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Virus Classification using Support Vector Machines (SVM)\n",
        "\n",
        "In this lab we will classify viruses into their sub families using their DNA sequence. This has potential applications in disease control where scientists can monitor the spread and mutations of viruses, and potentially try to identify whether when a benign virus will mutate to a malignant virus.\n",
        "\n",
        "For this lab we will mainly use Support Vector Machines ([SVM](https://scikit-learn.org/stable/modules/svm.html))"
      ]
    },
    {
      "metadata": {
        "id": "YldhiIUZSRTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import collections\n",
        "import csv\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "from scipy.stats import mode\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "seed = 976\n",
        "np.random.seed(seed)\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vl0AS5E4RQsE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The data\n",
        "\n",
        "The data can be found [here](https://drive.google.com/file/d/1tb1pvtUNqx3r4FVbNsRKum2hwaLOIYmO/view?usp=sharing). Click add to drive so that you can mount it later.\n"
      ]
    },
    {
      "metadata": {
        "id": "rINs2LCh43Zn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9cGvgDubCaEe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/Chosen_Data_clearToN.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZPajuUGqC_u0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data preparation\n",
        "\n",
        "For the data provided, we will only use the \"Sequences\" and the \"Label\" columns.\n",
        "\n",
        "**Sequences** refers to the DNA sequence of a virus sample.\n",
        "\n",
        "**Label** refers to the classification of the virus."
      ]
    },
    {
      "metadata": {
        "id": "xyrynFGv85K_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a dataframe that only have the two columns.\n",
        "\n",
        "How many kinds of viruses are we working with?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "All these viruses are different variations of Ebola viruses."
      ]
    },
    {
      "metadata": {
        "id": "vJd89780SRUB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#1 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ftsxDz5AFUSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each DNA string has different length. Plot the histogram of the length of the DNA."
      ]
    },
    {
      "metadata": {
        "id": "FU04USfPF9Nk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#2 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3x1Qp3wFGgAg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Handling input of different sizes (text)\n",
        "\n",
        "In machine learning, it is oftened assume that the input features are of the same length for each training data (since the model is a function which requires fixed-length inputs). However, in this case we have variable-length inputs. A naive solution would be to pad (add blank characters) so that the input will be the same length. This can sometimes create unwanted effects. For example in this case, most sequence will be padded to 38000 which causes the sequence after the 32000th position to be mostly blank characters for most data sample. Another solution would be to use models that can handle variable-length inputs, for example recurrent neural networks. In this homework we will use another powerful concept called, bag-of-words (BoW) representation."
      ]
    },
    {
      "metadata": {
        "id": "WV6rYVt9H-Jn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BoW\n",
        "\n",
        "BoW refers to the representation of character counts a string.\n",
        "\n",
        "For example, the string\n",
        "\n",
        "\"apple\"\n",
        "\n",
        "can be represented as counts below:\n",
        "\n",
        "Char | Counts\n",
        "--- | ---\n",
        "a | 1\n",
        "p | 2\n",
        "l | 1\n",
        "e | 1\n",
        "\n",
        "This representation is a bit too simple. People can add richness to the representation by counting every subsequence of length n as shown below:\n",
        "\n",
        "substring(n=2) | Counts\n",
        "--- | ---\n",
        "ap | 1\n",
        "pp | 1\n",
        "pl | 1\n",
        "le | 1\n",
        "\n",
        "This is called [n-gram](https://en.wikipedia.org/wiki/N-gram) representation (n=2). For the field of bioinformatics, this representation is usually refered to as [k-mer](https://en.wikipedia.org/wiki/K-mer) (k=2).\n"
      ]
    },
    {
      "metadata": {
        "id": "HkV8ySCHRlRF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train, validation, test split can be done using the function below. Note how the split is done one class at a time. This is done so that the portion of classes in each subset are the same. This is called stratified splitting.\n",
        "\n",
        "Sk-learn also have functions that accomplish this. You can look at it [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)."
      ]
    },
    {
      "metadata": {
        "id": "8D9eZrCBSRUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def splitTrainTest(data):\n",
        "  keyDatas = data[\"Label\"].value_counts().keys()\n",
        "  train = pd.DataFrame()\n",
        "  valid = pd.DataFrame()\n",
        "  test = pd.DataFrame()\n",
        "  chk = 0\n",
        "  for k in keyDatas:\n",
        "    tmp = data[data[\"Label\"]==k]\n",
        "    tmp_train, tmp_test = train_test_split(tmp, test_size=2/5, random_state=seed)\n",
        "    tmp_train, tmp_valid = train_test_split(tmp_train, test_size=1/6, random_state=seed)\n",
        "    train = train.append(tmp_train)\n",
        "    valid = valid.append(tmp_valid)\n",
        "    test = test.append(tmp_test)\n",
        "  return train, valid, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GwxCgJOnSRUf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train, df_valid, df_test = splitTrainTest(df)\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_valid = df_valid.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Em1iL4c8q4_H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Why should we do stratified splitting? (hint: think about how we do hyperparameter tuning).\n",
        "\n",
        "** Ans:**"
      ]
    },
    {
      "metadata": {
        "id": "LyTk5lJnTY2h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check if the data in each subset has the correct propotion for each class.\n",
        "\n",
        "What is the majority class? What is the minority class? What is the ratio in terms of number of training data between majority and minority classes?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "l6xPpXrJJpJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#3 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r5Raum4ASvet",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## N-gram generation\n",
        "\n",
        "Write a function that takes in a DNA sequence and return an n-gram of ATCG combinations.\n",
        "\n",
        "You can use [itertools.product](https://www.hackerrank.com/challenges/itertools-product/problem) to help create n-gram permutations.\n",
        "\n",
        "Note: besides ATCG, there is also the letter \"N.\" This is an unknown base. Ignore any n-gram that contains \"N.\"\n",
        "\n",
        "Hint: use a dictionary to keep track of counts for each n-gram."
      ]
    },
    {
      "metadata": {
        "id": "XV6OOdEiSRUk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def createGram(sequence, gram=5):\n",
        "  # sequence is the DNA sequence\n",
        "  # returns: np array where each entry is an n-gram count\n",
        "\n",
        "  ## TODO#4 ##\n",
        "  return ngramdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pA9bf7OSylWt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Try\n",
        "print(createGram('AGTCATC',1))\n",
        "print(createGram('AGTCATC',2))\n",
        "\n",
        "#You should get\n",
        "#[2 2 2 1]\n",
        "#[0 0 1 1 1 0 0 0 0 2 0 0 0 0 1 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqPZDrEpHE2y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "def createGram(sequence, gram=5):\n",
        "  nGram = dict.fromkeys([\"\".join(x) for x in itertools.product(\"ACTG\",repeat=gram) ],0)\n",
        "  for i in  range(len(sequence)-gram+1):\n",
        "    if \"N\" not in str([sequence[i:i+gram]]):\n",
        "      nGram[sequence[i:i+gram]]+=1\n",
        "  return np.array(list(nGram.values()))\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "ml0B8UwIAt1b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "createGramDataset() is a function that calls createGram to generate n-gram features for the entire dataset. For each training data, we take the n-gram features from a  **random** sub-sequence of length t (default value = 10000). \n",
        "\n",
        "It can also upsampling the data to deal with our imbalanced dataset. To generate extra training examples for the class with smaller amounts of training data we can take a larger number of sub-sequences per training data.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WtYvcrso8AfS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def createGramDataset(data, gram=5, length=10000, train=False):\n",
        "  datangram = []\n",
        "  label = []\n",
        "\n",
        "  if(train):\n",
        "    # Upsampling if training data\n",
        "    \n",
        "    # Assume first class has maximum amount\n",
        "    max_sam = data[\"Label\"].value_counts()[0]\n",
        "    # For each label\n",
        "    for i in data[\"Label\"].value_counts().keys():\n",
        "      tmp_virus = data[data[\"Label\"]==i]\n",
        "      tmp_virus = tmp_virus.reset_index(drop=True)\n",
        "      # Upsample by j times\n",
        "      for j in range(0, max_sam//len(tmp_virus)):\n",
        "        # For data point\n",
        "        for k in range(len(tmp_virus)):\n",
        "          # Select location of the sub-sequence\n",
        "          if(len(tmp_virus[\"Sequences\"][k])-length == 0):\n",
        "            rand_int = 0\n",
        "          else:\n",
        "            rand_int = np.random.randint(len(tmp_virus[\"Sequences\"][k])-length)\n",
        "          selected_sequence = tmp_virus[\"Sequences\"][k][rand_int:rand_int+length]\n",
        "          datangram.append(createGram(tmp_virus[\"Sequences\"][k]))\n",
        "          label.append(i)\n",
        "  else:\n",
        "    # For data point\n",
        "    for k in range(len(data)):\n",
        "      if (len(data[\"Sequences\"][k])-length == 0) :\n",
        "        rand_int = 0\n",
        "      else:\n",
        "        rand_int = np.random.randint(len(data[\"Sequences\"][k])-length)\n",
        "      selected_sequence = data[\"Sequences\"][k][rand_int:rand_int+length]\n",
        "      datangram.append(createGram(data[\"Sequences\"][k]))\n",
        "      label.append(data[\"Label\"][k])\n",
        "\n",
        "  return np.array(datangram), label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j3TBJVUrSRUo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, y_train = createGramDataset(df_train, 5, 20000, True)\n",
        "X_valid, y_valid = createGramDataset(df_valid, 5, 20000, False)\n",
        "X_test, y_test = createGramDataset(df_test, 5, 20000, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SGpBU-kISL33",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "buildlLabel() is a function that maps categorical values to numbers."
      ]
    },
    {
      "metadata": {
        "id": "Ks3KnaP-SRU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def buildLabel(y_str_label,key_virus_label):\n",
        "  nplabel = []\n",
        "  for lab in y_str_label:\n",
        "    for i in np.arange(len(key_virus_label)):\n",
        "      if(lab == key_virus_label[i]):\n",
        "        nplabel.append(i+1)\n",
        "        break\n",
        "  return np.array(nplabel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DuAFbBKGSRU9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "key_virus = df[\"Label\"].value_counts().index\n",
        "y_train = buildLabel(y_train,key_virus)\n",
        "y_valid = buildLabel(y_valid,key_virus)\n",
        "y_test = buildLabel(y_test,key_virus)\n",
        "print(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "21KP26jfSRVD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Shuffle the training data so that classes will not appear together.\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4J_Hajs0LeaT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A function that spits out several metrics\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "def eval(y_pred,y_test):\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  f1 = f1_score(y_test, y_pred, average='macro') \n",
        "  prec = precision_score(y_test, y_pred, average='macro') \n",
        "  recall = recall_score(y_test, y_pred, average='macro') \n",
        "  return acc, f1, prec, recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lgk1u8r27w_T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Kernal (SVC)\n",
        "\n",
        "Use [SVM](https://scikit-learn.org/stable/modules/svm.html) in sk-learn with linear kernel to train a model with default parameters. Report the accuracy, f1, precision, and recall using the function eval()."
      ]
    },
    {
      "metadata": {
        "id": "Xl6_3mbNHCVT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#5 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I_BGQR047pmn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "clf_Linear = SVC(kernel='linear')\n",
        "clf_Linear.fit(X_train, y_train)\n",
        "y_pred_linear = clf_Linear.predict(X_test)\n",
        "\n",
        "acc, f1, precision, recall = eval(y_pred_linear,y_test)\n",
        "print('accuracy :',acc)\n",
        "print('f1 :',f1)\n",
        "print('precision :',precision)\n",
        "print('recall :',recall)\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "qfscSivm8EFA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Polynomial Kernel\n",
        "\n",
        "Try polynomail with polynomial degree 2, 3, and 4. The degree of the polynomial is typically a hyperparameter to tune. You may find the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) helpful.\n",
        "\n",
        "Report the difference on the validation and the test sets.\n",
        "\n",
        "Which one should you use?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "U71STOoq7VOR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#6 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRCp0dtaVSDu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Radial Basis Kernel (RBF)\n",
        "\n",
        "Finally, try RBF kernel.\n",
        "\n",
        "The parameter gamma refers to the size of our Gaussian kernel. A higher Gamma means smaller Guassian, and vice versa. In other words, the larger gamma is, the closer other examples must be to be affected.\n",
        "\n",
        "Try different gamma values and plot the accuracy of the training, validation, and the test set as a function of the parameter Gamma.\n",
        "\n",
        "Use Gamma = $10^x$ where $x$ is from {-4, -3, ..., 3, 4}"
      ]
    },
    {
      "metadata": {
        "id": "tCAxMyD1SRVK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#7 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bg5SSLsBBrr1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After you finish the plot, do you think we need to further tune the hyperparameter gamma?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "What is the relation between overfitting and the hyperparameter gamma?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "Which kernel is the best for this dataset?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "*The type of kernel that should be used depends on the dataset. This is also a kind of hyperparameter. Also make sure you tune the hyperparameters for each kernel. *"
      ]
    },
    {
      "metadata": {
        "id": "pNGnZCk5C96w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Finding the best 2000 \n",
        "\n",
        "DNA sequencing can also be done on part of the sequence. This can significantly reduce the cost of DNA sequencing. Find the best sub-sequent of length 2000, so that we can reduce the cost of sequencing.\n",
        "\n",
        "Suppose according to your biologist friend, there are 2 regions of significance:\n",
        "\n",
        "1.   Position 6000-8000\n",
        "2.   Position 8000-10000\n",
        "\n",
        "Which region is the better one?"
      ]
    },
    {
      "metadata": {
        "id": "IwMKbeLlEGjg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Example code of how to get 8000:10001\n",
        "df_cut = df.copy()\n",
        "sub_Sequences = [x[8000:10001] for x in df_cut['Sequences'].values]\n",
        "df_cut['sub_Sequences'] = sub_Sequences\n",
        "df_cut = df_cut[['sub_Sequences','Label']]\n",
        "\n",
        "df_cut.rename(columns={'sub_Sequences': 'Sequences'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPY3KXY4Fe0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#8 ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1A02H2uRGwok",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Which region is the better one?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "FuGTGGAbUaPB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Slack penalty C\n",
        "\n",
        "Another hyperparameter that should be tuned is the slack penalty. In practice this should be tuned according to each Kernel, but we have left them at the default values until now.\n",
        "\n",
        "Slack values refer to regularization term that allows some examples to be misclassified. This reduces overfitting. A low C has high regularization and makes the decision surface smooth, while a high C aims at classifying all training examples correctly (low slack).\n",
        "\n",
        "Another thing to note is the number of support vectors. The decision boundary of SVMs are built using support vectors. Kernel SVM relies on the calculation over all support vectors, so the more the support vectors the higher the runtime. This is also one of the reason why SVMs which do very well in small datasets do not see much practical usage. The bigger the dataset, the bigger amount of support vectors, resulting in higher runtime."
      ]
    },
    {
      "metadata": {
        "id": "q2DNXtWoJv5K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we will look at the relationship between the slack penalty, the number of support vectors, and the runtime.\n",
        "\n",
        "Train a RBF SVM (with default parameters), use C = $10^x$ where $x$ is from {-4, -3, ..., 3, 4}.\n",
        "\n",
        "Monitor the amount of support vectors (clf.n_support_) and the runtime for each value C.\n",
        "\n",
        "You can monitor the runtime by\n",
        "\n",
        "```\n",
        "start_time = time.time()\n",
        "# run 10 times to get the average\n",
        "for runs in range(10):\n",
        "    clf.fit(X_train_key_seq, y_train_key_seq)\n",
        "end_time = time.time()\n",
        "avg_time = (start_time - end_time)/10\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "gUUph7O1Gd3K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#9 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L9bzXIBCODAD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "df_train, df_valid, df_test = splitTrainTest(df)\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_valid = df_valid.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "X_train_key_seq, y_train_key_seq = createGramDataset(df_train, 5, 20000, True)\n",
        "X_valid_key_seq, y_valid_key_seq = createGramDataset(df_valid, 5, 20000, False)\n",
        "X_test_key_seq, y_test_key_seq = createGramDataset(df_test, 5, 20000, False)\n",
        "\n",
        "y_train_key_seq = buildLabel(y_train_key_seq,key_virus)\n",
        "y_valid_key_seq = buildLabel(y_valid_key_seq,key_virus)\n",
        "y_test_key_seq = buildLabel(y_test_key_seq,key_virus)\n",
        "\n",
        "X_train_key_seq, y_train_key_seq = shuffle(X_train_key_seq, y_train_key_seq, random_state=seed)\n",
        "\n",
        "train_acc = [] \n",
        "test_acc = []\n",
        "number_sv = []\n",
        "runtime = []\n",
        "\n",
        "a = range(-4,4)\n",
        "\n",
        "for idx,val in enumerate(a):\n",
        "  clf = SVC(gamma='auto',C=10**val)\n",
        "  start_time = time.time()\n",
        "  for runs in range(10):\n",
        "    clf.fit(X_train_key_seq, y_train_key_seq)\n",
        "  end_time = time.time()\n",
        "  y_pred_train = clf.predict(X_train_key_seq)\n",
        "  y_pred = clf.predict(X_test_key_seq)\n",
        "  \n",
        "  number_sv.append(np.sum(clf.n_support_))\n",
        "  acc_train, _, _, _ = evluate(y_pred_train,y_train_key_seq)\n",
        "  acc_test, _, _, _ = evluate(y_pred,y_test_key_seq)\n",
        "  train_acc.append(acc_train)\n",
        "  test_acc.append(acc_test)\n",
        "  runtime.append((end_time - start_time)/10)\n",
        "  \n",
        "plt.plot(a, np.array(train_acc), 'r', a, np.array(test_acc), 'b')\n",
        "plt.legend(('train', 'test'),loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(a, np.array(number_sv), 'r')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(a, np.array(runtime), 'r')\n",
        "plt.show()\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "x6nHHa0eNSNI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "According to the graphs what is the relationship between\n",
        "\n",
        "- Overfitting and C\n",
        "- C and number of support vectors\n",
        "- number of support vectors and runtime\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "NiNsvaKPOs67",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (Optional) New-virus identification using one-class SVM ([NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html))\n",
        "\n",
        "It is often very important to be able to identify when a new type of virus emerges. This type of problem is called Anomaly detection in machine leanring literature. Anomaly detection can be framed as a one class problem, where we reject every data point that is too far from the group of known classes.\n",
        "\n",
        "You can try leaving one classes of viruses out and create a classifier using the rest of the classes. Create a NuSVC model to identify whether the left out virus is a new type or not.\n",
        "\n",
        "Things to think about:\n",
        "\n",
        "- How to split the training, validation, and test data in this case?\n",
        "- How unique is each class? Can we quantify it?\n",
        "- Can we identify which pair of classes are closer than the rest?\n",
        "- Given time stamps of the samples, can we map out how these viruses evolve from each family?"
      ]
    },
    {
      "metadata": {
        "id": "KI6LcPI2QbVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}