{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4_part2_visualization_and_evaluation(student)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pataweepr/applyML_vistec_2019/blob/master/hw4.5_visualization_and_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qUBnrELdC8J2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Visualization and Evaluation\n",
        "\n",
        "In this lab we will look into ways to visualize our data via t-sne in order to get more insights about our data. We will also look into another important aspect of machine learning, evaluation.\n",
        "\n",
        "We will use the same data from the virus classification lab."
      ]
    },
    {
      "metadata": {
        "id": "Vl0AS5E4RQsE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The data preparation\n",
        "\n",
        "The data can be found [here](https://drive.google.com/file/d/1tb1pvtUNqx3r4FVbNsRKum2hwaLOIYmO/view?usp=sharing). Click add to drive so that you can mount it later. The following section only repeats the same stuff we did in the previous lab. Just keep running the code until the next section."
      ]
    },
    {
      "metadata": {
        "id": "YldhiIUZSRTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import collections\n",
        "import csv\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "from scipy.stats import mode\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "seed = 976\n",
        "np.random.seed(seed)\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rINs2LCh43Zn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9cGvgDubCaEe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/Chosen_Data_clearToN.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJd89780SRUB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df[[\"Sequences\", \"Label\"]]\n",
        "key_virus = df[\"Label\"].value_counts().index\n",
        "print(key_virus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8D9eZrCBSRUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def splitTrainTest(data):\n",
        "  keyDatas = data[\"Label\"].value_counts().keys()\n",
        "  train = pd.DataFrame()\n",
        "  valid = pd.DataFrame()\n",
        "  test = pd.DataFrame()\n",
        "  chk = 0\n",
        "  for k in keyDatas:\n",
        "    tmp = data[data[\"Label\"]==k]\n",
        "    tmp_train, tmp_test = train_test_split(tmp, test_size=2/5, random_state=seed)\n",
        "    tmp_train, tmp_valid = train_test_split(tmp_train, test_size=1/6, random_state=seed)\n",
        "    train = train.append(tmp_train)\n",
        "    valid = valid.append(tmp_valid)\n",
        "    test = test.append(tmp_test)\n",
        "  return train, valid, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GwxCgJOnSRUf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train, df_valid, df_test = splitTrainTest(df)\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_valid = df_valid.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XV6OOdEiSRUk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def createGram(sequence, gram=5):\n",
        "  # sequence is the DNA sequence\n",
        "  # returns: np array where each entry is an n-gram count\n",
        "  nGram = dict.fromkeys([\"\".join(x) for x in itertools.product(\"ACTG\",repeat=gram) ],0)\n",
        "  for i in  range(len(sequence)-gram+1):\n",
        "    if \"N\" not in str([sequence[i:i+gram]]):\n",
        "      nGram[sequence[i:i+gram]]+=1\n",
        "  return np.array(list(nGram.values()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WtYvcrso8AfS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def createGramDataset(data, gram=5, length=10000, train=False):\n",
        "  datangram = []\n",
        "  label = []\n",
        "\n",
        "  if(train):\n",
        "    # Upsampling if training data\n",
        "    \n",
        "    # Assume first class has maximum amount\n",
        "    max_sam = data[\"Label\"].value_counts()[0]\n",
        "    # For each label\n",
        "    for i in data[\"Label\"].value_counts().keys():\n",
        "      tmp_virus = data[data[\"Label\"]==i]\n",
        "      tmp_virus = tmp_virus.reset_index(drop=True)\n",
        "      # Upsample by j times\n",
        "      for j in range(0, max_sam//len(tmp_virus)):\n",
        "        # For data point\n",
        "        for k in range(len(tmp_virus)):\n",
        "          # Select location of the sub-sequence\n",
        "          if(len(tmp_virus[\"Sequences\"][k])-length == 0):\n",
        "            rand_int = 0\n",
        "          else:\n",
        "            rand_int = np.random.randint(len(tmp_virus[\"Sequences\"][k])-length)\n",
        "          selected_sequence = tmp_virus[\"Sequences\"][k][rand_int:rand_int+length]\n",
        "          datangram.append(createGram(tmp_virus[\"Sequences\"][k]))\n",
        "          label.append(i)\n",
        "  else:\n",
        "    # For data point\n",
        "    for k in range(len(data)):\n",
        "      if (len(data[\"Sequences\"][k])-length == 0) :\n",
        "        rand_int = 0\n",
        "      else:\n",
        "        rand_int = np.random.randint(len(data[\"Sequences\"][k])-length)\n",
        "      selected_sequence = data[\"Sequences\"][k][rand_int:rand_int+length]\n",
        "      datangram.append(createGram(data[\"Sequences\"][k]))\n",
        "      label.append(data[\"Label\"][k])\n",
        "\n",
        "  return np.array(datangram), label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j3TBJVUrSRUo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, y_train = createGramDataset(df_train, 5, 20000, True)\n",
        "X_valid, y_valid = createGramDataset(df_valid, 5, 20000, False)\n",
        "X_test, y_test = createGramDataset(df_test, 5, 20000, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ks3KnaP-SRU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def buildLabel(y_str_label,key_virus_label):\n",
        "  nplabel = []\n",
        "  for lab in y_str_label:\n",
        "    for i in np.arange(len(key_virus_label)):\n",
        "      if(lab == key_virus_label[i]):\n",
        "        nplabel.append(i+1)\n",
        "        break\n",
        "  return np.array(nplabel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4J_Hajs0LeaT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "def eval(y_pred,y_test):\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  f1 = f1_score(y_test, y_pred, average='macro') \n",
        "  prec = precision_score(y_test, y_pred, average='macro') \n",
        "  recall = recall_score(y_test, y_pred, average='macro') \n",
        "  return acc, f1, prec, recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DuAFbBKGSRU9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = buildLabel(y_train,key_virus)\n",
        "y_valid = buildLabel(y_valid,key_virus)\n",
        "y_test = buildLabel(y_test,key_virus)\n",
        "# Shuffle the training data so that classes will not appear together.\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QCIAn1qi5o_t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visuzliation with t-SNE\n",
        "\n",
        "Since our training features is very high dimensional (how many dimensions?), it is hard to make sense of what our model is doing. It is also hard to get a sense of which class is similar to each other, especially for our data.\n",
        "\n",
        "One way is to try to visualize the data using dimensionality reduction techniques. The goal is to map a high dimensional feature vector into 2 or 3 dimensions so that we can plot and see what is going on.\n",
        "\n",
        "[t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) is a visualization technique that can produce low dimensionality data. The goal is to have data that are closed together in the high dimensional space still be closed together in the low dimensional space."
      ]
    },
    {
      "metadata": {
        "id": "p59Y2zkzNHeb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use t-SNE to generate X_embedded, use two components, 1000 iteration, and a random_state = seed. Plot low dimensionality data using the code provided."
      ]
    },
    {
      "metadata": {
        "id": "6pFbJuhANUPp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "## TODO#1 ##\n",
        "# Code to do TSNE here. One line.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PLVHx53pVWW5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Draw the data\n",
        "colors = ['r','g','b','c','y','k']\n",
        "for i in range(1,7):\n",
        "  label_i = (y_train == i) \n",
        "  plt.scatter(X_embedded[label_i,0],X_embedded[label_i,1],c = colors[i-1],label=key_virus[i-1])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yU79kaKLOBNG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "According to the plot, which class should be the hardest to classify?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "_mmBucoBN_8-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One of the main things to note about t-SNE is that the visualization changes according to the initialization. Try a different random seed and observe the difference in the embedding plot."
      ]
    },
    {
      "metadata": {
        "id": "IzwKMI2sNzAO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#2 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T-6MbKmsOr0R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The embedding does not only depends on the intialization, since it is a gradient-based optimization, we need to specify the number of iterations as well.\n",
        "\n",
        "Visualized the embeddings (using the provided random seed) for iteration = \\[250, 260, 270, ..., 330, 340, 350\\]. "
      ]
    },
    {
      "metadata": {
        "id": "GA_t9LXCITQN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#3 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-o9ZV31ePWTa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unlike previous hyperparameter tuning we did, picking the number of iterations and the initialization, need to be done manually (as in looking at the visualizations). This is the annoying part about t-SNE. However, usually the same conclusions should emerge for most configurations."
      ]
    },
    {
      "metadata": {
        "id": "Ub9Ui5FqP6Rp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3 dimensional t-SNE\n",
        "\n",
        "We can also visualize t-SNE using 3 dimensions. However, to plot it on a screen, we need to specify the view angle when we plot using matplotlib. Study the following code which shows how to do so."
      ]
    },
    {
      "metadata": {
        "id": "cdMI9z1Zqu4M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_embedded_2 = TSNE(n_components=3,n_iter=15000,random_state = seed).fit_transform(X_train)\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "colors = ['r','g','b','c','y','k']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NDHaCik8wBFd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This code view the plot in two different angles.\n",
        "\n",
        "# 120\n",
        "ii = 120\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = Axes3D(fig)\n",
        "\n",
        "print('print ', ii)\n",
        "\n",
        "ax.view_init(elev=10., azim=ii)\n",
        "for i in range(1,7):\n",
        "  label_i = (y_train == i) \n",
        "  ax.scatter(X_embedded_2[label_i,0],X_embedded_2[label_i,1],X_embedded_2[label_i,2],c = colors[i-1])\n",
        "plt.show()\n",
        "print('-----------------------------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "# 300\n",
        "ii = 300\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = Axes3D(fig)\n",
        "\n",
        "print('print ', ii)\n",
        "\n",
        "ax.view_init(elev=10., azim=ii)\n",
        "for i in range(1,7):\n",
        "  label_i = (y_train == i) \n",
        "  ax.scatter(X_embedded_2[label_i,0],X_embedded_2[label_i,1],X_embedded_2[label_i,2],c = colors[i-1])\n",
        "plt.show()\n",
        "print('-----------------------------------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBOBcLQEBon2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Tensorboard projector](https://projector.tensorflow.org/) is a service provided by Google that lets you interactively play with tSNE and PCA to visualize the data. You can try it by downloading the file `virusngrams.tsv` and `viruslabels.tsv` and upload it using the button load data."
      ]
    },
    {
      "metadata": {
        "id": "JYD9Dm5KRgEx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.savetxt('/content/gdrive/My Drive/virusngrams.tsv',X_train,delimiter='\\t')\n",
        "np.savetxt('/content/gdrive/My Drive/viruslabels.tsv',y_train,delimiter='\\t',fmt='%d')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kj659HErCVpg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "In the next sections we will look into how we can get more insight from our evaluation than just accuracy scores via two tools:\n",
        "\n",
        "1. confusion matrix\n",
        "2. RoC curve"
      ]
    },
    {
      "metadata": {
        "id": "mEBJAFLTC10g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix\n",
        "\n",
        "In this section, we will look at confusion matrices. Confusion matrices decribe how each instance are classified as. The diagonal refers to correct classification, while the off-diagonals refer to mis-classificaiton.\n",
        "\n",
        "The code below create a linear SVM classifier (just like the previous lab)"
      ]
    },
    {
      "metadata": {
        "id": "8CHQnjDUDTqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf_Linear = SVC(kernel='linear')\n",
        "clf_Linear.fit(X_train, y_train)\n",
        "y_pred_linear = clf_Linear.predict(X_test)\n",
        "\n",
        "acc, f1, precision, recall = eval(y_pred_linear,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R21r4fciGBtA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use the function [`confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)  to compute the classification results for each category. Each row refer to each true class, while each column refer to the predicted label."
      ]
    },
    {
      "metadata": {
        "id": "OpOnU6T9GMjZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#4 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "raS01lKIP3mq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "confuse_test_linear = confusion_matrix(y_test, y_pred_linear)\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "JkYQmh_XHSNF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What does the number in row 2, column 1 (0-index) signify?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "From the confusion matrix which class is the hardest to classify? Does it agree with your observation when you did t-SNE visualization?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "VH2fQUZcHhU4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also normalize the confusion matrix so that each row sums to one. This turns each row to a conditional probability (Given true class, what's the probability to predict as each class)."
      ]
    },
    {
      "metadata": {
        "id": "aK8W7YSbHgn7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#5 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FXm_gRMWQTyE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "confuse_test_linear_norm = confuse_test_linear/confuse_test_linear.sum(axis = 1)\n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "QAYFigloQkgC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can visualize the confusion matrices using sns.heatmap() via a dataframe as shown below:"
      ]
    },
    {
      "metadata": {
        "id": "h3f2KB0DQjrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_linear = pd.DataFrame(data=confuse_test_linear, index=key_virus, columns = key_virus)\n",
        "ax = sns.heatmap(df_linear, annot=True, fmt=\"g\")\n",
        "plt.show()\n",
        "\n",
        "df_linear_norm = pd.DataFrame(data=confuse_test_linear_norm, index=key_virus, columns = key_virus)\n",
        "ax = sns.heatmap(df_linear_norm, annot=True, fmt=\"g\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jbU04jslQYBj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Between the normalized and the unnormalized confusion matrices which one do you prefer? Why?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "P7CkpN_DRUvz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compare the confusion matrix between a linear kernel and the RBF kernel (gamma = 10**-5)."
      ]
    },
    {
      "metadata": {
        "id": "MwnHAPeMRUC5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#6 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDsADV_NR0cX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If all type of errors are of equal importance, which model is better?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "Are there any kind of situations where the linear model would be the prefered model? Come up with a scenario.\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "xaEwqmbWSYjt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In general, most machine learning model will treat each kind of errors equally. However, in real usage scenarios, some kind of errors might be more serious. This can have a large effect on choosing classifiers. To introduce different weights for different kinds of errors, we usually need specialized code or loss functions which is beyond the scope of this lab."
      ]
    },
    {
      "metadata": {
        "id": "bqqyJ1V7TfAt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RoC curve for binary classification\n",
        "\n",
        "Given a function of a binary classifier $h(x)$, we ca perform classification by comparing $h(x)$ against a threshold $t$.\n",
        "\n",
        "But where does this $t$ come from? One way to select $t$ is to use the RoC curve which plots the FPR against the TPR. In this section we will create an RoC curve for the binary classification whether a virus is HKU or not."
      ]
    },
    {
      "metadata": {
        "id": "8NLs6rSGq0j2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create the new labels\n",
        "y_test_new = np.zeros(len(y_test),dtype ='int') \n",
        "y_train_new = np.zeros(len(y_train),dtype ='int')\n",
        "y_valid_new = np.zeros(len(y_valid),dtype ='int')\n",
        "\n",
        "y_test_new[y_test == 3] = 1\n",
        "y_train_new[y_train == 3] = 1\n",
        "y_valid_new[y_valid == 3] = 1\n",
        "\n",
        "key_virus_new = ['others','hku']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_wWAz4BsYY2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train a simple linear SVC. Then, use svc.dicision_function to get the value of the classifer $h(x)$, put it under a variable named, pred_func_linear."
      ]
    },
    {
      "metadata": {
        "id": "ISFdDBTVqnN2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#7 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IZ_-9pOfsljx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What value should $h(x)$ be for the HKU class?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "ovwUW1rZtYAg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use the function [roc_curve()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) to get the points on the RoC curve. The function will compare the predicted values against multiple threshold values and evaluate the FPR and TPR. You should get 3 arrays.\n",
        "\n",
        "1. FPR - the false positive rate at each threshold\n",
        "2. TPR - the true postive rate at each threshold\n",
        "3. THR - the threshold used"
      ]
    },
    {
      "metadata": {
        "id": "tt7a7btTtRnW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#8 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8vAne9V7uEhE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary>SOLUTION HERE!</summary>\n",
        "      <pre>\n",
        "        <code>\n",
        "fpr_linear, tpr_linear, thresholds_linear = roc_curve(y_test_new,y_pred_linear)\n",
        "plt.plot(fpr_linear,tpr_linear,label=\"linear\")\n",
        "plt.legend()\n",
        "plt.show()        \n",
        "        </code>\n",
        "      </pre>\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "Xk9WJWLMuTlZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Equal Error Rate (EER)\n",
        "\n",
        "The equal error rate is where the two kinds of error FPR and miss rate (1-TPR) are equal.\n",
        "\n",
        "What is the EER of this RoC? At what threshold?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "If you want at least 95% recall, which threshold would you pick?\n",
        "\n",
        "** Ans: **\n",
        "\n",
        "Given that a false alarm is twice as costly as a miss, which threshold would you pick?\n",
        "\n",
        "** Ans: **"
      ]
    },
    {
      "metadata": {
        "id": "8SxFF1fqzjjx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#9 ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XCcx44_TwT-c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Comparing RoCs\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fjNtErp7wlmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The main usage of RoC is comparing the performance of different models. Since we have seen previously, the best model can depend on how we weight between different kinds of errors.\n",
        "\n",
        "Create different SVC models of your choosing (can be different kernels or just different hyperparameters). Plot the RoCs of the three models on the same plot."
      ]
    },
    {
      "metadata": {
        "id": "rWJzefFpw7ha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO#10 ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WHGmiL7Jw8TN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Which model would you use? Why? Be sure to state your assumption.\n",
        "\n",
        "** Ans: **"
      ]
    }
  ]
}